# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FntR8B8nR2k-z2LoBbyTBnSIkMpYOlk7
"""

"""# New Section"""

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import scipy

import warnings

warnings.filterwarnings("ignore")

from textblob import TextBlob
from sklearn import metrics
from mlxtend.plotting import plot_confusion_matrix

data = pd.read_csv('Full Comments.csv', error_bad_lines=False)

data.head(5)

data.shape

data['polarity'] = data['Comment'].apply(lambda x: TextBlob(x).sentiment.polarity)

data.head()

data = data.sample(frac=1).reset_index(drop=True)

data['pol_cat'] = 0

data['pol_cat'][data.polarity > 0] = 1
data['pol_cat'][data.polarity <= 0] = -1

data.head()

data['pol_cat'].value_counts()

data_pos = data[data['pol_cat'] == 1]
data_pos = data_pos.reset_index(drop=True)

data_neg = data[data['pol_cat'] == -1]
data_neg = data_neg.reset_index(drop=True)

# data_neutral = data[data['pol_cat'] == 0]
# data_neutral = data_neutral.reset_index(drop = True)

data_pos.head()

data_pos.shape

data_neg.head()

data_neg['Comment'][20]

data.pol_cat.value_counts().plot.bar()
data.pol_cat.value_counts()

data['Comment'] = data['Comment'].str.lower()

data['Comment'].head()

data.describe()

data.info()

data['Comment'][0]

data['Comment'][0].strip()

import nltk

nltk.download("stopwords")

nltk.download("punkt")

from nltk.corpus import stopwords
from nltk import word_tokenize
import string
import re
import nltk

print(stopwords.words('english'))

stop_words = set(stopwords.words('english'))

data['Comment'] = data['Comment'].str.strip()

train = data.copy()

train['Comment'] = train['Comment'].str.strip()

train['Comment'][0]


def remove_stopwords(line):
    word_tokens = word_tokenize(line)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    return " ".join(filtered_sentence)


data['stop_comments'] = data['Comment'].apply(lambda x: remove_stopwords(x))

data.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data['stop_comments'], data['pol_cat'], test_size=0.2,
                                                    random_state=324)

X_train.shape

X_test.shape

data['pol_cat'].value_counts()

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

vect = CountVectorizer()
tf_train = vect.fit_transform(X_train)
tf_test = vect.transform(X_test)

print(vect.vocabulary_)

vocab = vect.vocabulary_

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(tf_train, y_train)

lr.score(tf_train, y_train)

lr.score(tf_test, y_test)

expected = y_test
predicted = lr.predict(tf_test)

from mlxtend.plotting import plot_confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

cf = metrics.confusion_matrix(expected, predicted, labels=[1, -1])
print(cf)
## this command is only for google collab 
## in the below figure " 1 is actually showing -1 and 0 is actually showing 1"
fig, ax = plot_confusion_matrix(conf_mat=cf)

## It canbe used in any platform
# fig, ax = plot_confusion_matrix(conf_mat = cf, class_names = [1,-1])
plt.show()

from sklearn import metrics

print(metrics.classification_report(expected, predicted))
# print(metrics.confusion_matrix(expected,predicted))

from sklearn.metrics import f1_score

f1_score(expected, predicted, average='macro')
